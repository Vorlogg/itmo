{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuyLVDZ4s8r"
      },
      "source": [
        "# Токенизация текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ5Q6w2IFHA"
      },
      "source": [
        "**Ссылка**, на источник текста."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64uxt-07IEec"
      },
      "source": [
        "DATA_URL = \"http://az.lib.ru/t/tolstoj_a_k/text_0180.shtml\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwrBkeLnHuA1"
      },
      "source": [
        "Нам понадобятся следующие библиотеки: **PyYAML**, **RNNMorph**, а также **NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Fr5swwYfz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d42cbe-98aa-4c1a-e398-60db67eeb700"
      },
      "source": [
        "! pip install -q PyYaml==5.3.1\n",
        "! pip install -q rnnmorph==0.4.0\n",
        "! pip install -q nltk==3.2.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 4.0MB/s \n",
            "\u001b[?25h  Building wheel for PyYaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 3.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.2MB 45.7MB/s \n",
            "\u001b[?25h  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbxMKqhPH1Dk"
      },
      "source": [
        "Создаём объект морфологического анализатора **RNNMorph**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24zMUhvi99AV"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59q1L9p0H9K9"
      },
      "source": [
        "Скачиваем текст, по которому будем производить дальнейший анализ, с помощью **urllib**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uW0fw_h-Pft"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "raw_text = resource.read().decode(resource.headers.get_content_charset()) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-hSPOjDo4sdh",
        "outputId": "9f81b324-41d0-4433-fe49-8eddbe105163"
      },
      "source": [
        "raw_text[:200]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<html>\\r\\n<head>\\r\\n<title>Lib.ru/Классика: Толстой Алексей Константинович. Семья вурдалака</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n\\r\\n<center>\\r\\n\\r\\n<h2><a href=/t/tolstoj_a_k/>Толстой Алексей Константинович</a><br>\\r\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiLHQNSITAt"
      },
      "source": [
        "Загруженный текст содержит в себе HTML-теги, от которых нужно избавиться. Для этого, воспользуемся **BeautifulSoup**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We4LkyUMPfuq"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(raw_text, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "cleaned_text = soup.get_text()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "lOD8PJnG4rbl",
        "outputId": "3c0d9c36-61ec-4e4f-b838-013dc84c81e8"
      },
      "source": [
        "cleaned_text[:200]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nLib.ru/Классика: Толстой Алексей Константинович. Семья вурдалака\\n\\n\\n\\nТолстой Алексей Константинович\\r\\nСемья вурдалака\\n\\n\\nLib.ru/Классика:\\n\\r\\n\\n\\n[Регистрация]\\n \\n\\r\\n\\r\\n\\r\\n[Найти] \\r\\n[Рейтинги]\\r\\n[Обсуждения]\\r\\n['"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fYYb5hIpnY"
      },
      "source": [
        "После удаление HTML-тегов, в тексте всё еще осталось множество специальных символов, но они не помешают легко токенизировать текст. С помощью библиотеки **NLTK** разбиваем текст на предложения и слова (**токены**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "hRNu7jPvN6G_",
        "outputId": "722d6475-aa13-4d48-c4ac-dc71ca99c3fc"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]\n",
        "\"A total of %d 'sentences'\" % len(tokenized_sentences)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"A total of 577 'sentences'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRU4KEBAIyYT"
      },
      "source": [
        "С помощью метода **str.isalpha()** из стандартной библиотеки **Python** выделим только буквенные токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U5HH2CDPVUM",
        "outputId": "d644ae8d-7981-4d44-9667-15951e673dba"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions = [[pred.normal_form for pred in sent if pred.normal_form.isalpha()] \n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\") ]\n",
        "predictions[-11:-10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "sentences: 100%|██████████| 577/577 [00:00<00:00, 43203.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['не',\n",
              "  'помнить',\n",
              "  'что',\n",
              "  'произойти',\n",
              "  'ещё',\n",
              "  'но',\n",
              "  'когда',\n",
              "  'я',\n",
              "  'прийти',\n",
              "  'в',\n",
              "  'себя',\n",
              "  'быть',\n",
              "  'уже',\n",
              "  'вполне',\n",
              "  'светлый',\n",
              "  'я',\n",
              "  'лежать',\n",
              "  'на',\n",
              "  'дорога',\n",
              "  'а',\n",
              "  'рядом',\n",
              "  'издыхать',\n",
              "  'мой',\n",
              "  'конь']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGDhxhNJtTz"
      },
      "source": [
        "Должны получиться следующие значения:\n",
        "\n",
        "*   Предложений: **577~**\n",
        "*   Токенов: **8621~** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwK_qRbw6sac",
        "outputId": "8f33488b-acfe-4b3a-f23e-8f88edf4de80"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5jL4sWyKUnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee0a209-94dd-4e0e-f516-8a748aad5a5f"
      },
      "source": [
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
        "len(non_uniq_tokens) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8621"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mci9Nd5hKuJP"
      },
      "source": [
        "Используя **non_uniq_tokens**, стоп-слова для русского языка из библиотеки **nltk.corpus.stopwords** и **nltk.FreqDist**, вычислим, какую долю среди 100 самых частотных токенов в произведении составляют токены, которых нет в списке стоп-слов **nltk.corpus.stopwords**. \n",
        "\n",
        "Если на **100** наиболее часто-встречаемых слов, приходится **25**, которые входят в стоп-лист, значит доля значимых слов составит **0.75**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbtLqkLKfZC",
        "outputId": "a6d98c8f-e4f4-4b26-d26b-e7858899c486"
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и', 'в', 'во', 'не', 'что']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdbB95YwtSl"
      },
      "source": [
        "Должно получиться **0.49~** (допустимая погрешность **0.1**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXAKqYXDNqp-",
        "outputId": "4d3c93de-5ff5-4816-d250-9d7795377fdf"
      },
      "source": [
        "no_stops = [token for token in non_uniq_tokens if not token in stopwords.words(\"russian\")]\n",
        "print(\"Доля значимых слов: %.2f\" %((len(non_uniq_tokens)-len(no_stops))/len(non_uniq_tokens)))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Доля значимых слов: 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChyAdk2Ovx1"
      },
      "source": [
        "Вычислим, сколько токенов встречается в тексте **строго больше** 50 раз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ1UvPZdWz5r",
        "outputId": "17a9e482-bc22-4a6c-d225-8b6391be3dd3"
      },
      "source": [
        "uniq_tokens = dict(FreqDist(samples=no_stops))\n",
        "greater_than_fifty = [i for i in uniq_tokens.items() if i[-1] > 50]\n",
        "print(len(greater_than_fifty))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6HZ2w3yxJEh"
      },
      "source": [
        "Проверьте себя: должно получиться значение 22 (возможно небольшое расхождение)\n"
      ]
    }
  ]
}